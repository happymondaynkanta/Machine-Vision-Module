{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, concatenate, GlobalAveragePooling2D, Dense, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'D:\\C5\\C6\\MV_Seminar Week 5\\data_mv\\train'\n",
    "test_dir = r'D:\\C5\\C6\\MV_Seminar Week 5\\data_mv\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',  # Assuming labels are provided as integers\n",
    "    color_mode='rgb'  # Assuming images are grayscale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the test data\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_generator.next()\n",
    "x_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Inception Module from Sratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defind the inception module (Module A)\n",
    "\n",
    "def inception_block(input_shape, filter_1x1):\n",
    "    \n",
    "    # First branch\n",
    "    branch_1_conv_1x1 = Conv2D(128, (1,1), padding='same', activation='relu')(input_shape)\n",
    "\n",
    "    # Second branch\n",
    "    branch_2_conv_1x1 = Conv2D(64, (1,1), padding='same', activation='relu')(input_shape)\n",
    "    branch_2_conv_3x3 = Conv2D(192, (3,3), padding='same', activation='relu')(branch_2_conv_1x1)\n",
    "    branch_2_conv_1x1 = Conv2D(filter_1x1, (1,1), padding='same', activation='relu')(branch_2_conv_3x3)\n",
    "\n",
    "    # Third branch\n",
    "    branch_3_conv_1x1 = Conv2D(64, (1,1), padding='same', activation='relu')(input_shape)\n",
    "    branch_3_conv_5x5 = Conv2D(96, (5,5), padding='same', activation='relu')(branch_3_conv_1x1 )\n",
    "    branch_3_conv_1x1 = Conv2D(64, (1,1), padding='same', activation='relu')(branch_3_conv_5x5)\n",
    "\n",
    "    branch_output = concatenate([branch_1_conv_1x1, branch_2_conv_1x1, branch_3_conv_1x1], axis=-1)\n",
    "    \n",
    "    return branch_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the lines of code for the Inception module (Module A):\n",
    "\n",
    "```python\n",
    "# Defind the inception module (Module A)\n",
    "def inception_block(input_shape, filter_1x1):\n",
    "    \n",
    "    # First branch\n",
    "    branch_1_conv_1x1 = Conv2D(128, (1,1), padding='same', activation='relu')(input_shape)\n",
    "\n",
    "    # Second branch\n",
    "    branch_2_conv_1x1 = Conv2D(64, (1,1), padding='same', activation='relu')(input_shape)\n",
    "    branch_2_conv_3x3 = Conv2D(192, (3,3), padding='same', activation='relu')(branch_2_conv_1x1)\n",
    "    branch_2_conv_1x1 = Conv2D(filter_1x1, (1,1), padding='same', activation='relu')(branch_2_conv_3x3)\n",
    "\n",
    "    # Third branch\n",
    "    branch_3_conv_1x1 = Conv2D(64, (1,1), padding='same', activation='relu')(input_shape)\n",
    "    branch_3_conv_5x5 = Conv2D(96, (5,5), padding='same', activation='relu')(branch_3_conv_1x1 )\n",
    "    branch_3_conv_1x1 = Conv2D(64, (1,1), padding='same', activation='relu')(branch_3_conv_5x5)\n",
    "\n",
    "    branch_output = concatenate([branch_1_conv_1x1, branch_2_conv_1x1, branch_3_conv_1x1], axis=-1)\n",
    "    \n",
    "    return branch_output\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **First Branch (`branch_1_conv_1x1`):**\n",
    "   - `branch_1_conv_1x1` is a 1x1 convolution with 128 filters applied to the input shape.\n",
    "   - The convolution is followed by batch normalization and a ReLU activation function.\n",
    "\n",
    "2. **Second Branch (`branch_2_conv_1x1`, `branch_2_conv_3x3`, `branch_2_conv_1x1`):**\n",
    "   - `branch_2_conv_1x1` is a 1x1 convolution with 64 filters applied to the input shape.\n",
    "   - `branch_2_conv_3x3` is a 3x3 convolution with 192 filters applied to the output of the previous 1x1 convolution.\n",
    "   - `branch_2_conv_1x1` is another 1x1 convolution with `filter_1x1` filters applied to the output of the 3x3 convolution.\n",
    "   - Each convolution is followed by batch normalization and a ReLU activation function.\n",
    "\n",
    "3. **Third Branch (`branch_3_conv_1x1`, `branch_3_conv_5x5`, `branch_3_conv_1x1`):**\n",
    "   - `branch_3_conv_1x1` is a 1x1 convolution with 64 filters applied to the input shape.\n",
    "   - `branch_3_conv_5x5` is a 5x5 convolution with 96 filters applied to the output of the previous 1x1 convolution.\n",
    "   - `branch_3_conv_1x1` is another 1x1 convolution with 64 filters applied to the output of the 5x5 convolution.\n",
    "   - Each convolution is followed by batch normalization and a ReLU activation function.\n",
    "\n",
    "4. **Concatenate Branch Outputs (`branch_output`):**\n",
    "   - The outputs of the three branches (`branch_1_conv_1x1`, `branch_2_conv_1x1`, `branch_3_conv_1x1`) are concatenated along the last axis (axis=-1).\n",
    "   - The concatenation creates the final output of the Inception module, combining features extracted by different convolutional filters.\n",
    "\n",
    "5. **Return (`return branch_output`):**\n",
    "   - The concatenated output is returned from the function.\n",
    "\n",
    "In summary, this Inception module creates a complex set of features by applying different convolutional filters with varying kernel sizes (1x1, 3x3, 5x5) to the input shape. The concatenated output captures diverse information, enhancing the representational power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = Input(shape=(128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(64, (7,7), activation='relu')(input_shape)\n",
    "x = MaxPooling2D(2,2)(x)\n",
    "x = Conv2D(64, (3,3), activation='relu')(x)\n",
    "x = Conv2D(32, (1,1), activation='relu')(x)\n",
    "x = inception_block(x, 64)\n",
    "x = MaxPooling2D(2,2)(x)\n",
    "x = inception_block(x, 64)\n",
    "x = MaxPooling2D(2,2)(x)\n",
    "x = inception_block(x, 64)\n",
    "x = MaxPooling2D(2,2)(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Dense(3, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Model(inputs=input_shape, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_1.fit(x_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model_1.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Residual Network of ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization\n",
    "from keras.layers import ReLU, Add, GlobalAveragePooling2D, Dense,MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(input_shape, filters, strides=1):\n",
    "    #\n",
    "    shortcut = input_shape\n",
    "\n",
    "    x = Conv2D(filters, (1,1), strides=1, padding='same')(input_shape)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv2D(filters, (3,3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv2D(filters*4, (1,1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # Skip connection\n",
    "    if strides !=1 or shortcut.shape[-1]!= filters*4:\n",
    "        shortcut = Conv2D(filters*4, (1,1), strides=strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    # Add the shortcut to the main path\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the lines of code for the residual block without the skip connection:\n",
    "\n",
    "```python\n",
    "def res_block(input_shape, filters, strides=1):\n",
    "    # Save the input shape as the shortcut (identity) connection\n",
    "    shortcut = input_shape\n",
    "\n",
    "    # First convolution block\n",
    "    x = Conv2D(filters, (1,1), strides=1, padding='same')(input_shape)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Second convolution block\n",
    "    x = Conv2D(filters, (3,3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Third convolution block (expansion)\n",
    "    x = Conv2D(filters*4, (1,1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Skip connection\n",
    "    if strides != 1 or shortcut.shape[-1] != filters*4:\n",
    "        shortcut = Conv2D(filters*4, (1,1), strides=strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    # Add the shortcut to the main path\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Save Input Shape as Shortcut Connection:**\n",
    "   - `shortcut = input_shape`: This line saves the input tensor as the shortcut connection. The shortcut connection is used to add the original input to the main path later in the code.\n",
    "\n",
    "2. **First Convolution Block (`x = Conv2D(filters, (1,1), strides=1, padding='same')(input_shape)`):**\n",
    "   - This is a 1x1 convolution applied to the input shape.\n",
    "   - Batch normalization and ReLU activation functions are applied to the output.\n",
    "\n",
    "3. **Second Convolution Block (`x = Conv2D(filters, (3,3), padding='same')(x)`):**\n",
    "   - This is a 3x3 convolution applied to the output of the first convolution block.\n",
    "   - Batch normalization and ReLU activation functions are applied to the output.\n",
    "\n",
    "4. **Third Convolution Block (Expansion, `x = Conv2D(filters*4, (1,1), padding='same')(x)`):**\n",
    "   - This is a 1x1 convolution applied to the output of the second convolution block, intended for expansion (increasing the number of filters by a factor of 4).\n",
    "   - Batch normalization is applied to the output.\n",
    "\n",
    "5. **Skip Connection:**\n",
    "   - `if strides != 1 or shortcut.shape[-1] != filters*4:`: This condition checks if the strides are not equal to 1 or if the number of channels in the shortcut does not match the expected number of channels after the third convolution block.\n",
    "   - If the condition is true, a 1x1 convolution is applied to the shortcut to match the dimensions.\n",
    "   - Batch normalization is applied to the shortcut.\n",
    "\n",
    "6. **Add Shortcut to Main Path (`x = Add()([x, shortcut])`):**\n",
    "   - The shortcut connection is added to the main path, facilitating the skip connection.\n",
    "   - The combined result is passed through a ReLU activation function.\n",
    "\n",
    "7. **Return (`return x`):**\n",
    "   - The final output tensor is returned from the function.\n",
    "\n",
    "In summary, this residual block applies three convolutional blocks to the input tensor and introduces a skip connection to facilitate the flow of gradients during training. The skip connection helps address the vanishing gradient problem in deep neural networks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Let's break down the lines of code that involve the skip connection in a residual block:\n",
    "\n",
    "```python\n",
    "# Skip connection\n",
    "if strides != 1 or shortcut.shape[-1] != filters * 4:\n",
    "    shortcut = Conv2D(filters * 4, (1, 1), strides=strides, padding='same')(shortcut)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "# Add the shortcut to the main path\n",
    "x = Add()([x, shortcut])\n",
    "x = ReLU()(x)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Skip Connection Check (`if strides != 1 or shortcut.shape[-1] != filters * 4`):**\n",
    "   - This line checks whether a skip connection (also known as a shortcut connection) is needed. A skip connection allows the gradient to flow more easily during training, mitigating the vanishing gradient problem.\n",
    "   - The condition checks two things:\n",
    "     - `strides != 1`: This condition checks if the convolutional block is downsampling the spatial dimensions. If `strides` is not equal to 1, it means the block is performing downsampling.\n",
    "     - `shortcut.shape[-1] != filters * 4`: This checks if the number of channels (the last dimension of the shape) in the shortcut connection is not equal to four times the number of filters in the current block. If they are not equal, it means the shortcut needs to be adjusted to match the dimensions of the main path.\n",
    "\n",
    "2. **Shortcut Adjustment (`shortcut = Conv2D(filters * 4, (1, 1), strides=strides, padding='same')(shortcut)`):**\n",
    "   - If the skip connection check is True, this line adjusts the shortcut connection by applying a 1x1 convolution with `filters * 4` filters. This convolution is applied with the same strides as the main path convolution (controlled by the `strides` parameter).\n",
    "   - The 1x1 convolution is used to adjust the number of channels in the shortcut to match the dimensions of the main path.\n",
    "\n",
    "3. **Batch Normalization on Shortcut (`shortcut = BatchNormalization()(shortcut)`):**\n",
    "   - After adjusting the shortcut, batch normalization is applied to the shortcut. Batch normalization helps stabilize and accelerate training by normalizing the input to a layer.\n",
    "\n",
    "4. **Add the Shortcut to the Main Path (`x = Add()([x, shortcut])`):**\n",
    "   - The adjusted shortcut is added to the output of the main path. This is the essence of the skip connection, where the original input (shortcut) is added to the transformed input of the main path. This helps in the flow of gradients during backpropagation.\n",
    "\n",
    "5. **ReLU Activation (`x = ReLU()(x)`):**\n",
    "   - After adding the shortcut, a Rectified Linear Unit (ReLU) activation is applied to the combined output. ReLU introduces non-linearity to the model by zeroing out negative values.\n",
    "\n",
    "In summary, this block of code implements a residual block with a skip connection. The skip connection is adjusted based on the conditions specified, and the adjusted shortcut is added to the main path before applying a ReLU activation. This structure allows for the training of deeper networks more effectively by addressing the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = Input(shape=(128,128,3))\n",
    "\n",
    "# Build a simple ResNet with two residual blocks\n",
    "x = Conv2D(64, (7,7), strides=2, padding='same')(input_shape)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = MaxPooling2D(2,2)(x)\n",
    "x = res_block(x, filters=64, strides=1)\n",
    "x = res_block(x, filters=64, strides=1)\n",
    "x = MaxPooling2D(2,2)(x)\n",
    "x = res_block(x, filters=128, strides=1)\n",
    "x = res_block(x, filters=128, strides=1)\n",
    "x = MaxPooling2D(2,2)(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Dense(3, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model(inputs=input_shape, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_2.fit(x_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model_1.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the models into Creating the Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the lines of code for creating the ensemble model:\n",
    "\n",
    "```python\n",
    "# Concatenate the outputs of the two models\n",
    "concatenated_output = concatenate([model_1.output, model_2.output])\n",
    "\n",
    "# Apply dense layers to the concatenated output\n",
    "x = Dense(100, activation='relu')(concatenated_output)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "\n",
    "# Create the final ensemble model output layer\n",
    "ensemble_model = Dense(3, activation='relu')(x)\n",
    "\n",
    "# Build the ensemble model\n",
    "model = Model(inputs=[model_1.input, model_2.input], outputs=ensemble_model)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Concatenate Model Outputs (`concatenate([model_1.output, model_2.output])`):**\n",
    "   - `concatenate` is a function that concatenates tensors along a specified axis. Here, it concatenates the outputs of `model_1` and `model_2` along the last axis (`axis=-1`).\n",
    "   - `model_1.output` and `model_2.output` are the final output tensors of the respective models.\n",
    "\n",
    "2. **Dense Layers Applied to Concatenated Output (`x = Dense(100, activation='relu')(concatenated_output)` and `x = Dense(50, activation='relu')(x)`):**\n",
    "   - Two dense layers are applied successively to the concatenated output tensor (`concatenated_output`).\n",
    "   - The first dense layer has 100 units with a ReLU activation function.\n",
    "   - The second dense layer has 50 units with a ReLU activation function.\n",
    "   - These dense layers introduce non-linear transformations to the concatenated features, allowing the model to learn complex representations.\n",
    "\n",
    "3. **Create Final Ensemble Model Output Layer (`ensemble_model = Dense(3, activation='relu')(x)`):**\n",
    "   - Another dense layer is added to produce the final ensemble model output.\n",
    "   - It has 3 units, representing the number of classes in the classification problem.\n",
    "   - The ReLU activation function is applied to the output.\n",
    "\n",
    "4. **Build the Ensemble Model (`model = Model(inputs=[model_1.input, model_2.input], outputs=ensemble_model)`):**\n",
    "   - The `Model` class is used to create the ensemble model.\n",
    "   - The inputs to the model are the input layers of `model_1` and `model_2`.\n",
    "   - The output is the `ensemble_model` created by stacking dense layers on the concatenated output.\n",
    "\n",
    "In summary, this code creates an ensemble model by concatenating the outputs of two existing models (`model_1` and `model_2`). Dense layers are then applied to the concatenated output to introduce non-linear transformations, and the final output layer is created for classification. The resulting ensemble model is built using the Keras `Model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_output= concatenate([model_1.output, model_2.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(100, activation='relu')(concatenated_output)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "ensemble_model= Dense(3, activation='relu')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the ensemble model\n",
    "model = Model(inputs=[model_1.input, model_2.input] , outputs= ensemble_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the ensemble model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ensemble model\n",
    "history = model.fit([x_train, x_train], y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ensemble model on the test set\n",
    "test_loss_ensemble, test_acc_ensemble = model.evaluate([x_test,x_test], y_test)\n",
    "print(\"Ensemble Test accuracy:\", test_acc_ensemble)\n",
    "print(\"Ensemble Test loss:\", test_loss_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "del model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You need to install numba using 'pip install numba'\n",
    "\n",
    "from numba import cuda\n",
    "\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
