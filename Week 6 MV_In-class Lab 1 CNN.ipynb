{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "#### 1. `import numpy as np`: Imports the NumPy library with the alias `np`, which is commonly used for numerical operations.\n",
    "\n",
    "#### 2. `import os`: Imports the Python standard library module `os` for working with the file system.\n",
    "\n",
    "#### 3. `from keras.preprocessing.image import ImageDataGenerator`: Imports the `ImageDataGenerator` class from Keras, which is used for data preprocessing and augmentation.\n",
    "\n",
    "#### import matplotlib.pyplot as plt: This line imports the pyplot module from Matplotlib and gives it the alias plt. Matplotlib is a widely used library for creating data visualizations, including charts and graphs.\n",
    "\n",
    "#### from keras.models import Sequential: This line imports the Sequential class from Keras. A Sequential model is a linear stack of layers and is commonly used for creating feedforward neural networks.\n",
    "\n",
    "#### from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense: This line imports specific layer types from Keras. Here are the explanations for each:\n",
    "\n",
    "#### Conv2D: This is a 2D convolutional layer used for processing image data. It's typically followed by activation functions and max-pooling layers in Convolutional Neural Networks (CNNs).\n",
    "#### MaxPooling2D: This is a 2D max-pooling layer used to downsample the feature maps in a CNN, reducing their spatial dimensions.\n",
    "#### Flatten: This layer is used to flatten the output from the previous layers into a 1D array. It's often used before fully connected layers.\n",
    "#### Dense: This is a fully connected layer, also known as a dense layer, in a neural network. It connects all neurons from the previous layer to the current layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the path to the dataset\n",
    "\n",
    "#### 4. `train_dir` and `test_dir`: These variables store the file paths to the training and test data directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'D:\\C5\\C6\\MV_Seminar Week 5\\data_mv\\train'\n",
    "test_dir = r'D:\\C5\\C6\\MV_Seminar Week 5\\data_mv\\test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the training data\n",
    "\n",
    "#### 5. `image_size = (28, 28)`: Defines the desired image dimensions (height and width) for the dataset.\n",
    "\n",
    "#### 6. `batch_size = 32`: Sets the batch size for loading and processing images during training and testing.\n",
    "\n",
    "#### 7. `train_datagen = ImageDataGenerator(rescale=1.0/255)`: Initializes an `ImageDataGenerator` for the training data and rescales the pixel values to be between 0 and 1 by dividing by 255. This rescaling is a common preprocessing step to normalize the data.\n",
    "\n",
    "#### 8. `train_generator = train_datagen.flow_from_directory(...)`: Creates a generator for the training data using the `flow_from_directory` method. This generator loads images from the specified directory and applies the preprocessing transformations defined earlier. It also specifies the image size, batch size, class mode (assuming integer labels), and color mode (grayscale in this case).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',  # Assuming labels are provided as integers\n",
    "    color_mode='rgb'  # Assuming images are grayscale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the test data\n",
    "\n",
    "#### 9. `test_datagen = ImageDataGenerator(rescale=1.0/255)`: Initializes another `ImageDataGenerator` for the test data with the same rescaling operation.\n",
    "\n",
    "#### 10. `test_generator = test_datagen.flow_from_directory(...)`: Creates a generator for the test data in a similar way to the training generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the test data\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train, validation, and test sets\n",
    "\n",
    "#### 11. `x_train, y_train = train_generator.next()`: Loads and returns the next batch of training data and labels from the training generator. This is done using the `next()` method.\n",
    "\n",
    "#### 12. `x_test, y_test = test_generator.next()`: Loads and returns the next batch of test data and labels from the test generator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_generator.next()\n",
    "x_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape) \n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use 20% of the training data for validation\n",
    "\n",
    "#### 13. `validation_split = 0.2`: Specifies the percentage of data you want to use for validation (20% in this case).\n",
    "\n",
    "#### 14. `split_index = int((1 - validation_split) * len(x_train))`: Calculates the index where the training data should be split into training and validation sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.2\n",
    "split_index = int((1 - validation_split) * len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the data beyond the split index for validation set\n",
    "\n",
    "#### 15. `x_validation = x_train[split_index:]`   and    `y_validation = y_train[split_index:]`: Separates the data beyond the split index for validation set. `x_train` and `y_train` now contain the training data, and `x_validation` and `y_validation` contain the validation data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation = x_train[split_index:]\n",
    "y_validation = y_train[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_validation.shape)\n",
    "print(y_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncate the training data up to the split index\n",
    "\n",
    "#### 16. `x_train = x_train[:split_index]` and `y_train = y_train[:split_index]`: Truncate the training data up to the split index. Now, `x_train` and `y_train` contain the training data, and the data has been successfully split into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:split_index]\n",
    "y_train = y_train[:split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Convolutional Neural Network model\n",
    "\n",
    "#### Model Definition:\n",
    "\n",
    "#### model = Sequential([...]): Here, a Sequential model is defined using a list of layers. The model is constructed sequentially, where each layer is added one after the other.\n",
    "\n",
    "#### Convolutional Layers:\n",
    "\n",
    "#### Conv2D(16, (3,3), activation='relu', input_shape=(128, 128, 3)): This line adds a convolutional layer with 16 filters of size 3x3, ReLU activation function, and an input shape of (128, 128, 3). It's the first layer in the model.\n",
    "#### MaxPooling2D((2,2)): This line adds a 2x2 max-pooling layer immediately after the first convolutional layer.\n",
    "#### Conv2D(32, (3,3), activation='relu'): This line adds another convolutional layer with 32 filters of size 3x3 and ReLU activation.\n",
    "#### MaxPooling2D((2,2)): Another 2x2 max-pooling layer follows the second convolutional layer.\n",
    "#### Conv2D(64, (3,3), activation='relu'): This is the final convolutional layer in this model.\n",
    "#### Flattening and Fully Connected Layers:\n",
    "\n",
    "#### Flatten(): This line adds a Flatten layer that takes the 2D feature maps from the previous layers and flattens them into a 1D array. This prepares the data for the fully connected layers.\n",
    "#### Dense(100, activation='relu'): This line adds a fully connected layer with 100 neurons and ReLU activation.\n",
    "#### Dense(3, activation='softmax'): The last layer is a fully connected layer with 3 neurons, using softmax activation. It's the output layer for multi-class classification.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the CNN model\n",
    "\n",
    "#### Model Compilation:\n",
    "\n",
    "#### model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']): The model is compiled with the Adam optimizer, categorical cross-entropy loss (suitable for multi-class classification), and accuracy as the evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the CNN model for training\n",
    "\n",
    "#### Model Training:\n",
    "\n",
    "#### history = model.fit(x_train, y_train, validation_data=(x_validation, y_validation), epochs=20): The model is trained using the training data (x_train, y_train) with validation data (x_validation, y_validation) for 20 epochs. The training history, including loss and accuracy over epochs, is stored in the history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_validation, y_validation), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test Accuracy:',test_accuracy)\n",
    "print('Test loss:',test_loss )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plot training accuracy history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plot training loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
