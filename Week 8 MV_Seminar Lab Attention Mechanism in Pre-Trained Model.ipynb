{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, GlobalAveragePooling2D, Reshape, Dense, multiply, Lambda\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'D:\\C5\\C6\\MV_Seminar Week 5\\data_mv\\train'\n",
    "test_dir = r'D:\\C5\\C6\\MV_Seminar Week 5\\data_mv\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',  # Assuming labels are provided as integers\n",
    "    color_mode='rgb'  # Assuming images are grayscale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the test data\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_generator.next()\n",
    "x_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model (excluding top layers)\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(128, 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom top layers for classification\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(64, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add attention mechanism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This line creates a dense layer with a single unit and a tanh activation function. \n",
    "\n",
    "- This layer is responsible for computing attention weights. Each element in the input tensor x will be assigned a weight by this dense layer. \n",
    "- The tanh activation is chosen here to squash the output to the range [-1, 1], helping the network to learn attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing attention weights. Each element in the input tensor x will be assigned a weight by this dense layer\n",
    "attention = Dense(1, activation='tanh')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attention weights are reshaped to have dimensions (-1, 1, 1). This reshaping is done to match the shape of the original input tensor x, facilitating the subsequent element-wise multiplication.  This prepares the attention weights to be applied element-wise to the original feature representation.\n",
    "\n",
    "\n",
    "- In the line of code `attention = Reshape((-1, 1, 1))(attention)`, the tuple `(-1, 1, 1)` specifies the desired shape to which the `attention` tensor is being reshaped. Let's break down each dimension:\n",
    "\n",
    "- The first dimension, represented by `-1`, is a placeholder for an unknown size. The value of `-1` is inferred based on the size of the original tensor, such that the total number of elements remains constant. This is often used when you want to maintain the original size along that axis.\n",
    "\n",
    "- The second and third dimensions are explicitly set to `1`. This means that the reshaped tensor will have a size of 1 along these dimensions.\n",
    "\n",
    "The purpose of reshaping with `(-1, 1, 1)` is often to flatten the tensor along certain axes while preserving the total number of elements. Specifically, in this case:\n",
    "\n",
    "- The original tensor has some shape `(batch_size, height, width, channels)`.\n",
    "- After applying the attention mechanism, the tensor has been modified, and reshaping is applied to it.\n",
    "- The reshaping transforms the tensor into a shape of `(batch_size, 1, 1, 1)`.\n",
    "\n",
    "This reshaping might be necessary for compatibility with subsequent operations or layers in the neural network architecture. In the context of attention mechanisms, it is common to reshape attention weights to ensure that they can be easily applied to the original feature representation using element-wise multiplication.\n",
    "\n",
    "In summary, `(-1, 1, 1)` is used to reshape the tensor in such a way that it flattens along certain axes while maintaining the original size along other axes, and the specific size along the flattened axes is set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape attention to have dimensions (-1, 1, 1)\n",
    "attention = Reshape((-1, 1, 1))(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attention weights are applied element-wise to the original feature representation (x) using the multiply function. This step emphasizes or de-emphasizes certain elements based on their relevance determined by the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the attention weights element-wise to the original feature representation (x) \n",
    "attention = multiply([x, attention])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attention weights are summed along the channel axis (-2). \n",
    "\n",
    "- This aggregation step produces a single attention value for each spatial location in the input tensor.\n",
    "\n",
    "- In the line of code `attention = Lambda(lambda x: K.sum(x, axis=-2))(attention)`, several concepts are at play:\n",
    "\n",
    "1. **Lambda:**\n",
    "   - `Lambda` is a wrapper in Keras that allows you to use a custom function (specified using a lambda function) as a layer in your neural network.\n",
    "   - It is used to define a simple, one-line anonymous function.\n",
    "\n",
    "2. **lambda x:**\n",
    "   - This part is a lambda function, a concise way to define a small, inline function without explicitly naming it.\n",
    "   - In this case, the lambda function takes a single argument `x` and applies the operation defined in the function.\n",
    "\n",
    "3. **K.sum(x, axis=-2):**\n",
    "   - `K.sum` is a function from the Keras backend (`K`) that computes the sum of elements along a specified axis.\n",
    "   - In this case, it sums the elements along the axis specified by `-2`. The axis is counted from the last dimension, so `-2` corresponds to the channel axis in a typical 4D tensor representing images (batch_size, height, width, channels).\n",
    "\n",
    "4. **axis=-2:**\n",
    "   - This specifies the axis along which the summation operation should be performed.\n",
    "   - The value `-2` corresponds to the channel axis in a 4D tensor. It ensures that the attention values are summed along the channel dimension, producing a single attention value for each spatial location.\n",
    "\n",
    "Putting it all together, the line of code is essentially saying: \"Use a Lambda layer to apply a summation operation along the channel axis (-2) for the input tensor `x`.\"\n",
    "\n",
    "Example illustration:\n",
    "\n",
    "Suppose `attention` is a tensor with shape `(batch_size, height, width, channels)`. The lambda function is applied to this tensor, and the `K.sum(x, axis=-2)` operation sums the values along the channel axis, resulting in a tensor with shape `(batch_size, height, width, 1)`. Each element in the resulting tensor represents the aggregated attention value for a specific spatial location in the input tensor. This aggregated attention value captures the importance of different channels at each spatial location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attention weights are summed along the channel axis (-2) to produces a single attention value \n",
    "attention = Lambda(lambda x: K.sum(x, axis=-2))(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attention values are expanded along a new axis (-1). This step is necessary to match the expected input shape of the GlobalAveragePooling2D layer.\n",
    "\n",
    "In the line of code `attention = Lambda(lambda x: K.expand_dims(x, axis=-1))(attention)`, the operations involve the Keras backend functions `Lambda`, `K.expand_dims`, and the `axis` parameter. Let's break it down:\n",
    "\n",
    "1. **Lambda:**\n",
    "   - `Lambda` is a wrapper in Keras that allows you to use a custom function (specified using a lambda function) as a layer in your neural network.\n",
    "\n",
    "2. **lambda x:**\n",
    "   - This part is a lambda function, a concise way to define a small, inline function without explicitly naming it.\n",
    "   - In this case, the lambda function takes a single argument `x` and applies the operation defined in the function.\n",
    "\n",
    "3. **K.expand_dims(x, axis=-1):**\n",
    "   - `K.expand_dims` is a function from the Keras backend (`K`) that adds a new axis to a tensor at a specified position.\n",
    "   - In this case, it adds a new axis at the end of the tensor (`axis=-1`), effectively expanding the tensor shape.\n",
    "\n",
    "4. **axis=-1:**\n",
    "   - This specifies the axis at which the new dimension should be added. The value `-1` corresponds to the last axis of the tensor.\n",
    "\n",
    "Putting it all together, the line of code is essentially saying: \"Use a Lambda layer to apply the `K.expand_dims` operation to the input tensor `x`, adding a new dimension at the end of the tensor.\"\n",
    "\n",
    "Example illustration:\n",
    "\n",
    "Suppose `attention` is a tensor with shape `(batch_size, height, width)`. The lambda function is applied to this tensor, and the `K.expand_dims(x, axis=-1)` operation adds a new dimension at the end of the tensor, resulting in a tensor with shape `(batch_size, height, width, 1)`. The new dimension at the end represents the expanded axis. This expansion is often done to prepare the tensor for subsequent operations or layers that expect a certain number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attention values are expanded along a new axis (-1)\n",
    "attention = Lambda(lambda x: K.expand_dims(x, axis=-1))(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The resulting tensor from the attention mechanism is globally averaged along the spatial dimensions. This is a simple way to aggregate the attended features into a fixed-size representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the average of all the values along each channel\n",
    "attention = GlobalAveragePooling2D()(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the output layer is a dense layer with 3 units (assuming it's a classification task with three classes) and a softmax activation function. This layer produces the final probability distribution over the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = Dense(3, activation='softmax')(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new model\n",
    "model = Model(inputs=base_model.input, outputs=merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base layers to prevent their weights from being updated\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history=model.fit(x_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You need to install numba using 'pip install numba'\n",
    "\n",
    "from numba import cuda\n",
    "\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
